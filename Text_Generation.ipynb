{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "EBV4KAmLzHcw",
        "outputId": "f9e345ff-1903-4517-bcf5-1e9f75513205"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Length of text: 94275 characters\n",
            "the sonnets\n",
            "\n",
            "by william shakespeare\n",
            "\n",
            "from fairest creatures we desire increase,\n",
            "that thereby beauty's rose might never die,\n",
            "but as the riper should by time decease,\n",
            "his tender heir might bear his memory:\n",
            "but thou contracted to thine own bright eyes,\n",
            "feed'st thy light's flame with self-substantial fuel,\n",
            "making a famine where abundance lies,\n",
            "thy self thy foe, to thy sweet self too cruel:\n",
            "thou that art now the world's fresh ornament,\n",
            "and only herald to the gaudy spring,\n",
            "within thine own bud buriest\n",
            "Total characters: 94275\n",
            "Unique characters: 38\n",
            "Number of sequences: 31412\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/layers/rnn/rnn.py:200: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(**kwargs)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"sequential\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                        \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape               \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m        Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
              "│ lstm (\u001b[38;5;33mLSTM\u001b[0m)                          │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)                 │          \u001b[38;5;34m85,504\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense (\u001b[38;5;33mDense\u001b[0m)                        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m38\u001b[0m)                  │           \u001b[38;5;34m4,902\u001b[0m │\n",
              "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                         </span>┃<span style=\"font-weight: bold\"> Output Shape                </span>┃<span style=\"font-weight: bold\">         Param # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
              "│ lstm (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                          │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)                 │          <span style=\"color: #00af00; text-decoration-color: #00af00\">85,504</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">38</span>)                  │           <span style=\"color: #00af00; text-decoration-color: #00af00\">4,902</span> │\n",
              "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m90,406\u001b[0m (353.15 KB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">90,406</span> (353.15 KB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m90,406\u001b[0m (353.15 KB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">90,406</span> (353.15 KB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/30\n",
            "\u001b[1m197/197\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 135ms/step - loss: 2.8148 - val_loss: 2.1542\n",
            "Epoch 2/30\n",
            "\u001b[1m197/197\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m38s\u001b[0m 120ms/step - loss: 2.0644 - val_loss: 1.9684\n",
            "Epoch 3/30\n",
            "\u001b[1m197/197\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 122ms/step - loss: 1.8358 - val_loss: 1.8777\n",
            "Epoch 4/30\n",
            "\u001b[1m197/197\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 120ms/step - loss: 1.6937 - val_loss: 1.8235\n",
            "Epoch 5/30\n",
            "\u001b[1m197/197\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 114ms/step - loss: 1.5833 - val_loss: 1.8242\n",
            "Epoch 6/30\n",
            "\u001b[1m197/197\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 120ms/step - loss: 1.4834 - val_loss: 1.8279\n",
            "Epoch 7/30\n",
            "\u001b[1m197/197\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 130ms/step - loss: 1.3936 - val_loss: 1.8343\n",
            "Epoch 8/30\n",
            "\u001b[1m197/197\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 123ms/step - loss: 1.3021 - val_loss: 1.8666\n",
            "Epoch 9/30\n",
            "\u001b[1m197/197\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 120ms/step - loss: 1.2366 - val_loss: 1.8932\n",
            "Epoch 10/30\n",
            "\u001b[1m197/197\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 118ms/step - loss: 1.2054 - val_loss: 1.9113\n",
            "Epoch 11/30\n",
            "\u001b[1m197/197\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 114ms/step - loss: 1.1446 - val_loss: 1.9522\n",
            "Epoch 12/30\n",
            "\u001b[1m197/197\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 119ms/step - loss: 1.1069 - val_loss: 1.9834\n",
            "Epoch 13/30\n",
            "\u001b[1m197/197\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 121ms/step - loss: 1.0722 - val_loss: 2.0274\n",
            "Epoch 14/30\n",
            "\u001b[1m197/197\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 118ms/step - loss: 1.0307 - val_loss: 2.0714\n",
            "Epoch 15/30\n",
            "\u001b[1m197/197\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 119ms/step - loss: 1.0002 - val_loss: 2.0846\n",
            "Epoch 16/30\n",
            "\u001b[1m197/197\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 121ms/step - loss: 0.9805 - val_loss: 2.1251\n",
            "Epoch 17/30\n",
            "\u001b[1m197/197\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 121ms/step - loss: 0.9605 - val_loss: 2.1329\n",
            "Epoch 18/30\n",
            "\u001b[1m197/197\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 116ms/step - loss: 0.9112 - val_loss: 2.1784\n",
            "Epoch 19/30\n",
            "\u001b[1m197/197\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 122ms/step - loss: 0.9158 - val_loss: 2.1901\n",
            "Epoch 20/30\n",
            "\u001b[1m197/197\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 119ms/step - loss: 0.8888 - val_loss: 2.2321\n",
            "Epoch 21/30\n",
            "\u001b[1m197/197\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 118ms/step - loss: 0.8584 - val_loss: 2.2520\n",
            "Epoch 22/30\n",
            "\u001b[1m197/197\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 119ms/step - loss: 0.8663 - val_loss: 2.3117\n",
            "Epoch 23/30\n",
            "\u001b[1m197/197\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 121ms/step - loss: 0.8513 - val_loss: 2.3000\n",
            "Epoch 24/30\n",
            "\u001b[1m197/197\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 118ms/step - loss: 0.8111 - val_loss: 2.3469\n",
            "Epoch 25/30\n",
            "\u001b[1m197/197\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 119ms/step - loss: 0.8314 - val_loss: 2.3624\n",
            "Epoch 26/30\n",
            "\u001b[1m197/197\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 121ms/step - loss: 0.8051 - val_loss: 2.4416\n",
            "Epoch 27/30\n",
            "\u001b[1m197/197\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 121ms/step - loss: 0.8153 - val_loss: 2.3687\n",
            "Epoch 28/30\n",
            "\u001b[1m197/197\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 129ms/step - loss: 0.7578 - val_loss: 2.4232\n",
            "Epoch 29/30\n",
            "\u001b[1m197/197\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 120ms/step - loss: 0.7604 - val_loss: 2.4761\n",
            "Epoch 30/30\n",
            "\u001b[1m197/197\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 118ms/step - loss: 0.7712 - val_loss: 2.4938\n",
            "----- Original text excerpt -----\n",
            "e.\n",
            "how much more praise deserved thy beauty's use,\n",
            "if thou couldst answer 'this fair child of mine\n",
            "shall sum my count, and make my old excuse'\n",
            "proving his beauty by succession thine.\n",
            "this were to be new made when thou art old,\n",
            "and see thy blood warm when thou feel'st it cold.\n",
            "\n",
            "look in thy glass and tell the face thou viewest,\n",
            "now is the time that face should form another,\n",
            "whose fresh repair if now\n",
            "\n",
            "----- Generated text -----\n",
            "----- Generating with temperature 0.2\n",
            "----- Seed: \"shall i compare thee to a summer's day?\n",
            "\"\n",
            "shall i compare thee to a summer's day?\n",
            "mon thou are much me where thou art of hour,\n",
            "and thou mayst thee, for my self are starn,\n",
            "for though make my plears will sing slead,\n",
            "for is my love where thou are precign decaie,\n",
            "and therefore where lose thou are precive.\n",
            "for it the fair werst it in you,\n",
            "and therefore which poserst time days be divide,\n",
            "and therefore where tway,\n",
            "now my for my self theed a moreprow.  \n",
            "then i sceep thy self darts of h\n",
            "\n",
            "----- Generating with temperature 0.5\n",
            "----- Seed: \"shall i compare thee to a summer's day?\n",
            "\"\n",
            "shall i compare thee to a summer's day?\n",
            "man of thee, thou will my are scarsed,\n",
            "in my for my self both my resige your cries,\n",
            "o thowed mine eyes the strengy be time decaie:\n",
            "and i appears,  \n",
            "fut thine, and they fair, when their present.\n",
            "\n",
            "willh i all on me with the dare,\n",
            "and like a where with my beswined,\n",
            "and made they may seem my desert as to ime.\n",
            " \n",
            "for thee, for well you well you and make me,\n",
            "then i in thy pifter covie,\n",
            "was i brest though\n",
            "\n",
            "----- Generating with temperature 1.0\n",
            "----- Seed: \"shall i compare thee to a summer's day?\n",
            "\"\n",
            "shall i compare thee to a summer's day?\n",
            "ind thou mayst mine bist, and chide of yeared,\n",
            "thou tile, not with my self voil-gencelie,\n",
            "for me noth sloost,   \n",
            "as you are fairs,   \n",
            "then look once, for twite of thy summent.\n",
            "his fair let so is still yet for my judecerse,\n",
            "that will be fall sing, wherefore doth gend\n",
            "in they my well one, therefore to the time,\n",
            "thesefore wherefore and ail it my doth,\n",
            "not the good respont ackunt, jood, sterived day,\n",
            "\n",
            "\n",
            "----- Generating with temperature 1.5\n",
            "----- Seed: \"shall i compare thee to a summer's day?\n",
            "\"\n",
            "shall i compare thee to a summer's day?\n",
            "moke hand no ell, you, thougaim'x joought's bud.'s desined,\n",
            "and than war whay i'rovid, therebicdard\n",
            "weaps of a sun debast be oldpeds i nexreserdect:\n",
            "that 'gainst thee are bran wrecker agell link,\n",
            "of beauty excubs thy soightless thenst excel.o hand,  \n",
            "this stoulling me heatt) load, i veals a confeni de,\n",
            "alone thing? ressensee by trivil lewfon:'s brood,\n",
            "lif in are dreds do charthing as p;sul my:\n",
            "bur\n",
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"shall i compare thee to a summer's day?\\nmoke hand no ell, you, thougaim'x joought's bud.'s desined,\\nand than war whay i'rovid, therebicdard\\nweaps of a sun debast be oldpeds i nexreserdect:\\nthat 'gainst thee are bran wrecker agell link,\\nof beauty excubs thy soightless thenst excel.o hand,  \\nthis stoulling me heatt) load, i veals a confeni de,\\nalone thing? ressensee by trivil lewfon:'s brood,\\nlif in are dreds do charthing as p;sul my:\\nbur\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 1
        }
      ],
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import LSTM, Dense, Embedding\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "import random\n",
        "import requests  # To download the text dataset\n",
        "\n",
        "# 1. Load and preprocess the text data\n",
        "url = \"https://raw.githubusercontent.com/brunoklein99/deep-learning-notes/master/shakespeare.txt\"\n",
        "response = requests.get(url)\n",
        "text = response.text.lower()  # Convert to lowercase for consistency\n",
        "\n",
        "print(f\"Length of text: {len(text)} characters\")\n",
        "print(text[:500])  # Print first 500 characters to inspect\n",
        "\n",
        "# 2. Create character-level mappings\n",
        "chars = sorted(list(set(text)))\n",
        "char_indices = {c: i for i, c in enumerate(chars)}\n",
        "indices_char = {i: c for i, c in enumerate(chars)}\n",
        "\n",
        "print(f\"Total characters: {len(text)}\")\n",
        "print(f\"Unique characters: {len(chars)}\")\n",
        "\n",
        "# 3. Prepare training sequences\n",
        "maxlen = 40  # Length of input sequences\n",
        "step = 3     # Step size between sequences\n",
        "sentences = []  # Input sequences\n",
        "next_chars = []  # Output characters (labels)\n",
        "\n",
        "for i in range(0, len(text) - maxlen, step):\n",
        "    sentences.append(text[i:i + maxlen])\n",
        "    next_chars.append(text[i + maxlen])\n",
        "\n",
        "print(f\"Number of sequences: {len(sentences)}\")\n",
        "\n",
        "# Vectorize the data\n",
        "X = np.zeros((len(sentences), maxlen, len(chars)), dtype=np.bool)\n",
        "y = np.zeros((len(sentences), len(chars)), dtype=np.bool)\n",
        "\n",
        "for i, sentence in enumerate(sentences):\n",
        "    for t, char in enumerate(sentence):\n",
        "        X[i, t, char_indices[char]] = 1\n",
        "    y[i, char_indices[next_chars[i]]] = 1\n",
        "\n",
        "# 4. Build the LSTM model\n",
        "model = Sequential([\n",
        "    LSTM(128, input_shape=(maxlen, len(chars))),\n",
        "    Dense(len(chars), activation='softmax')\n",
        "])\n",
        "\n",
        "model.compile(optimizer=Adam(learning_rate=0.01),\n",
        "             loss='categorical_crossentropy')\n",
        "\n",
        "model.summary()\n",
        "\n",
        "# 5. Train the model\n",
        "history = model.fit(X, y, batch_size=128, epochs=30, validation_split=0.2)\n",
        "\n",
        "# 6. Text generation function with temperature\n",
        "def sample(preds, temperature=1.0):\n",
        "    preds = np.asarray(preds).astype('float64')\n",
        "    preds = np.log(preds) / temperature\n",
        "    exp_preds = np.exp(preds)\n",
        "    preds = exp_preds / np.sum(exp_preds)\n",
        "    probas = np.random.multinomial(1, preds, 1)\n",
        "    return np.argmax(probas)\n",
        "\n",
        "def generate_text(seed_text, length=400, temperature=1.0):\n",
        "    generated = seed_text\n",
        "    print(f'----- Generating with temperature {temperature}')\n",
        "    print(f'----- Seed: \"{seed_text}\"')\n",
        "\n",
        "    for i in range(length):\n",
        "        sampled = np.zeros((1, maxlen, len(chars)))\n",
        "        for t, char in enumerate(generated[-maxlen:]):\n",
        "            sampled[0, t, char_indices[char]] = 1.\n",
        "\n",
        "        preds = model.predict(sampled, verbose=0)[0]\n",
        "        next_index = sample(preds, temperature)\n",
        "        next_char = indices_char[next_index]\n",
        "\n",
        "        generated += next_char\n",
        "\n",
        "    print(generated)\n",
        "    print()\n",
        "    return generated\n",
        "\n",
        "# 7. Generate text with different temperatures\n",
        "seed_text = \"shall i compare thee to a summer's day?\\n\"\n",
        "print(\"----- Original text excerpt -----\")\n",
        "print(text[1000:1400])\n",
        "print(\"\\n----- Generated text -----\")\n",
        "\n",
        "# Generate with different temperature settings\n",
        "generate_text(seed_text, temperature=0.2)\n",
        "generate_text(seed_text, temperature=0.5)\n",
        "generate_text(seed_text, temperature=1.0)\n",
        "generate_text(seed_text, temperature=1.5)"
      ]
    }
  ]
}